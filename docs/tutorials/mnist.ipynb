{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN on MNIST\n",
    "\n",
    "This tutorial demonstrates how to build a simple convolutional neural network (CNN) with Inox, and train it to classify digits. It is intended for those who are new to JAX and Inox, or simply curious.\n",
    "\n",
    "Unlike [PyTorch](https://pytorch.org), which is a centralized framework, the [JAX](https://jax.readthedocs.io) ecosystem is distributed as a collection of packages, each tackling a well-defined task. This tutorial uses [Inox](https://inox.readthedocs.io) to build the network, [Optax](https://optax.readthedocs.io) to optimize the parameters, and [ðŸ¤— Datasets](https://huggingface.co/docs/datasets) to load the [MNIST](https://wikipedia.org/wiki/MNIST_database) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install jax[cpu] inox optax datasets[jax] tqdm\n",
    "\n",
    "import jax\n",
    "import inox\n",
    "import inox.nn as nn\n",
    "import optax\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "jax.config.update('jax_platform_name', 'cpu')\n",
    "jax.numpy.set_printoptions(suppress=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "JAX does not provide built-in datasets and dataloaders, as there are already many alternatives. We load the MNIST dataset using [ðŸ¤— Datasets](https://huggingface.co/docs/datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APAACzBVBJJwAO9dnp/wm8damu6Dw5dRjGf9IKw/+hkVPffCnWNJa7XVNV0Kxa1hErrNe/M2cnYqgElsAHpjkc1wlAODkV694W8c654t8M6n4TuvEctrrFw0cun3c0/lq+3AMJcDK5AyOeTkd+fPvGFn4gsvEtzF4m89tUG1ZJJjuMgUBVYN/EMKOe9YVXtK0bUtdvVs9LsZ7y4YgbIULYycZPoPc8V6lpfwh0/w7p66z8RdXj0y2z8llC4aWQ+mRn8lz9RXPfE3x1pvi46TYaPZTQadpMJghluWDSyrhQM9SMBe5Oc5NcBV7Tda1XRZJJNK1O8sXkG12tZ2iLD0JUjNQ3l9eahN517dT3MvTfNIXb16n6mq9Ff/2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA90lEQVR4AWNgGMyAWUhIqK5jvdSy/9/rQe5kgTlWjs3KRiAYxHsyKfDzxYMgFiOIAALDvfwQBsO/pK8Mz97fhPLAlNDtvyBwbNv3j8jCUHbAnOy/f89yM2jPwiLJwMc4628UqgQTnPvp/0eGFAQXLg5lcO/764YuhuArf3y4IAfmfoQwlBX44e/fckkMYaiA7q6/f6dJ45IViP3zdzcuSQaGn39/OkBl4WEL4euFmLIwXDuETav6lKfAIPy1DYucRNFdUPCe9MOUE3e6CpI6FogZSEKrwbFyOIATQ5v5mkcgXV9auVGlwK4NDGRguL75b88HVDla8QBFF16ADQA8sQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist = load_dataset('mnist')\n",
    "mnist['train'][0]['image']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform the images into NumPy arrays which are compatible with JAX and define a pre-processing procedure to rescale the pixel values to $[0, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_np = mnist.with_format('numpy')\n",
    "\n",
    "def process(x):\n",
    "    return x / 256"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Our model is a simple convolutional neural network. We define its architecture by a sequence of parametric functions, often called layers.\n",
    "\n",
    "A few remarks:\n",
    "\n",
    "1. Following JAX's random number generation (RNG) design choices, Inox layers like [`Linear`](inox.nn.linear.Linear) and [`Conv`](inox.nn.linear.Conv) require an RNG key for parameter initialization.\n",
    "\n",
    "2. Like [TensorFlow](https://www.tensorflow.org), Inox adopts a channel-last convention for axes, meaning that a batch of images is expected to have a shape $(N, H, W, C)$, where $C$ is the number of channels.\n",
    "\n",
    "3. [`Rearrange`](inox.nn.einops.Rearrange) and [`Repeat`](inox.nn.einops.Repeat) are thin wrappers around [einops](https://github.com/arogozhnikov/einops) that enable intuitive and efficient axis manipulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, key):\n",
    "        keys = jax.random.split(key, 4)\n",
    "\n",
    "        self.layers = [\n",
    "            nn.Repeat('H W -> H W C', C=1),\n",
    "            nn.Conv(in_channels=1, out_channels=4, kernel_size=[3, 3], key=keys[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv(in_channels=4, out_channels=4, kernel_size=[3, 3], key=keys[1]),\n",
    "            nn.MaxPool(window_size=[2, 2]),\n",
    "            nn.Rearrange('H W C -> (H W C)'),\n",
    "            nn.Linear(in_features=576, out_features=256, key=keys[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=10, key=keys[3]),\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        return jax.nn.softmax(self(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is a PyTree, that is just a nested collection of Python objects. Some of these objects are JAX arrays, like the convolution kernels, while others are arbitrary objects, like the pattern strings. Inox provides a nice representation for its modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  layers = [\n",
       "    Repeat(\n",
       "      lengths = {'C': 1},\n",
       "      pattern = 'H W -> H W C'\n",
       "    ),\n",
       "    Conv(\n",
       "      bias = Parameter(float32[4]),\n",
       "      dilation = [1, 1],\n",
       "      groups = 1,\n",
       "      kernel = Parameter(float32[3, 3, 1, 4]),\n",
       "      kernel_size = [3, 3],\n",
       "      padding = [(0, 0), (0, 0)],\n",
       "      stride = [1, 1]\n",
       "    ),\n",
       "    ReLU(),\n",
       "    Conv(\n",
       "      bias = Parameter(float32[4]),\n",
       "      dilation = [1, 1],\n",
       "      groups = 1,\n",
       "      kernel = Parameter(float32[3, 3, 4, 4]),\n",
       "      kernel_size = [3, 3],\n",
       "      padding = [(0, 0), (0, 0)],\n",
       "      stride = [1, 1]\n",
       "    ),\n",
       "    MaxPool(\n",
       "      padding = [(0, 0), (0, 0)],\n",
       "      stride = [2, 2],\n",
       "      window_size = [2, 2]\n",
       "    ),\n",
       "    Rearrange(\n",
       "      lengths = {},\n",
       "      pattern = 'H W C -> (H W C)'\n",
       "    ),\n",
       "    Linear(\n",
       "      bias = Parameter(float32[256]),\n",
       "      weight = Parameter(float32[576, 256])\n",
       "    ),\n",
       "    ReLU(),\n",
       "    Linear(\n",
       "      bias = Parameter(float32[10]),\n",
       "      weight = Parameter(float32[256, 10])\n",
       "    )\n",
       "  ]\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN(jax.random.key(0))\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our model is built, we can use it to make predictions. However, since it has not been trained yet, it is currently unable to classify the digits. In the next cell, you see that the probability it associates with each digit (0 to 9) is more or less uniform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.10137316, 0.09839383, 0.1020477 , 0.09809755, 0.09756261,\n",
       "       0.10181981, 0.09610692, 0.09600346, 0.11052851, 0.0980664 ],      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = mnist_np['train'][0]['image']\n",
    "y = mnist_np['train'][0]['label']\n",
    "\n",
    "model.predict(process(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quantify the quality of our model's predictions with their [cross entropy](https://wikipedia.org/wiki/Cross_entropy). For perfect predictions, the cross entropy is null, making it a good training objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(2.2845504, dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optax.softmax_cross_entropy_with_integer_labels(\n",
    "    logits=model(process(x)),\n",
    "    labels=y,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Now that we have an objective to minimize, we can start to train the parameters $\\phi$ of our model. We use the [`Module.partition`](inox.nn.module.Module.partition) method to split the static definition (structure, hyper-parameters, ...) of the module from its arrays (parameters, buffers, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '.layers[1].bias.value': float32[4],\n",
      "  '.layers[1].kernel.value': float32[3, 3, 1, 4],\n",
      "  '.layers[3].bias.value': float32[4],\n",
      "  '.layers[3].kernel.value': float32[3, 3, 4, 4],\n",
      "  '.layers[6].bias.value': float32[256],\n",
      "  '.layers[6].weight.value': float32[576, 256],\n",
      "  '.layers[8].bias.value': float32[10],\n",
      "  '.layers[8].weight.value': float32[256, 10]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "static, params = model.partition()\n",
    "\n",
    "print(inox.tree_repr(params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize an [Optax](https://optax.readthedocs.io) optimizer (Adam) for the parameters of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.adam(learning_rate=1e-3)\n",
    "opt_state = optimizer.init(params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A training step consists in computing the gradients of the loss $\\ell(\\phi)$, here the cross entropy, with respect to the parameters $\\phi$ using [`jax.grad`](jax.grad) and then updating the parameters according to the gradients. The whole procedure is compiled just-in-time (JIT) with [`jax.jit`](jax.jit) to make it as fast as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def step(params, opt_state, x, y):\n",
    "    def ell(params):\n",
    "        model = static(params)\n",
    "        logits = jax.vmap(model)(process(x))\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(logits, y)\n",
    "\n",
    "        return jax.numpy.mean(loss)\n",
    "\n",
    "    grads = jax.grad(ell)(params)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "\n",
    "    return params, opt_state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, to train the model, we iteratively apply our training step with random batches loaded from our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "937it [00:10, 87.52it/s]\n"
     ]
    }
   ],
   "source": [
    "loader = mnist_np['train'].shuffle(seed=0).iter(batch_size=64, drop_last_batch=True)\n",
    "\n",
    "for batch in tqdm(loader):\n",
    "    params, opt_state = step(params, opt_state, batch['image'], batch['label'])\n",
    "\n",
    "model = static(params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Now that the parameters of our model are trained, we use them to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APA4Lea6nSC3hkmmc4SONSzMfQAdadc2dzZSmK7tpoJB/BKhU/kahoq1pupXuj6hDf6dcyW13CSY5YzhlyCDj8CRXXWvxe8b20SxtrH2lVOQbqCOU9OmWUn/APVXUfEfxBqCfDzSNJ16S2uNd1JxqEqpbohtIMYjQbQBlsEnv1HpXj9Fdx8OvDNlqNxe+IdeVh4e0VPPucLnznyNkQ/3j1/LjOa57xPr9z4n8R3usXQ2vcSZVB0jQcKo9gABWRRXSxeOdXt/A0nhGAW0WnSzGaZ1j/ey8g7SxOMZA6AHjrXNUV//2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAzUlEQVR4AWNgGDaAEeKTkNRnP5a+uIPmLajkPQWg+OerUMknXWfALBYIP1X/mpahg8VjWQaGP68lGR5BJKE6wUoEDc+YMjD8uHVdKGca1AwMKvjvRSEMQaiA2Mv/wVAmE4aabNH3NzEEoQLWP//Z4ZJjaP23mxWXJOfZH1a45Bjq/m3DKef9+4MlLknhu/+W4ZJjPv3vtjIuSbV//3xxyck/+FeMHNYo6lr//TNBEUDi2H5Cl0QKWxsehrtfkBQzMEAjGyJ20fkdiiReDgBpETyQooNMkwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = mnist['test'][0]['image']\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.00001288, 0.        , 0.0004001 , 0.0006646 , 0.        ,\n",
       "       0.00000394, 0.        , 0.9988293 , 0.00000029, 0.00008877],      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = mnist_np['test'][0]['image']\n",
    "\n",
    "model.predict(process(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inox",
   "language": "python",
   "name": "inox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
